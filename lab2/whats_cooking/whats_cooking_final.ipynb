{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from scipy import interp\n",
    "from  matplotlib import pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics, preprocessing\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Read the data\n",
    "df = pd.read_json(\"~/Documents/AML/HW2/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cuisine</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>brazilian</th>\n",
       "      <td>467</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>british</th>\n",
       "      <td>804</td>\n",
       "      <td>804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cajun_creole</th>\n",
       "      <td>1546</td>\n",
       "      <td>1546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chinese</th>\n",
       "      <td>2673</td>\n",
       "      <td>2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filipino</th>\n",
       "      <td>755</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>french</th>\n",
       "      <td>2646</td>\n",
       "      <td>2646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greek</th>\n",
       "      <td>1175</td>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian</th>\n",
       "      <td>3003</td>\n",
       "      <td>3003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irish</th>\n",
       "      <td>667</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian</th>\n",
       "      <td>7838</td>\n",
       "      <td>7838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jamaican</th>\n",
       "      <td>526</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japanese</th>\n",
       "      <td>1423</td>\n",
       "      <td>1423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>korean</th>\n",
       "      <td>830</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican</th>\n",
       "      <td>6438</td>\n",
       "      <td>6438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moroccan</th>\n",
       "      <td>821</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russian</th>\n",
       "      <td>489</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>southern_us</th>\n",
       "      <td>4320</td>\n",
       "      <td>4320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish</th>\n",
       "      <td>989</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thai</th>\n",
       "      <td>1539</td>\n",
       "      <td>1539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vietnamese</th>\n",
       "      <td>825</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id ingredients\n",
       "             count       count\n",
       "cuisine                       \n",
       "brazilian      467         467\n",
       "british        804         804\n",
       "cajun_creole  1546        1546\n",
       "chinese       2673        2673\n",
       "filipino       755         755\n",
       "french        2646        2646\n",
       "greek         1175        1175\n",
       "indian        3003        3003\n",
       "irish          667         667\n",
       "italian       7838        7838\n",
       "jamaican       526         526\n",
       "japanese      1423        1423\n",
       "korean         830         830\n",
       "mexican       6438        6438\n",
       "moroccan       821         821\n",
       "russian        489         489\n",
       "southern_us   4320        4320\n",
       "spanish        989         989\n",
       "thai          1539        1539\n",
       "vietnamese     825         825"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The number of recipes per cuisine genre are represented below by aggregating the recipes  per cuisine\n",
    "numCuisineInst = df.groupby(['cuisine']).agg(['count'])\n",
    "numCuisineInst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The number of categories is 20\n",
    "len(numCuisineInst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Here we find that the number of rows is 39,774\n",
    "ingredients_list = df['ingredients'].as_matrix()\n",
    "len(ingredients_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6714"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Here we loop through every ingredient, and create a dictionary \n",
    "## of the ingredients to get only the unique ingredients\n",
    "ingredientDict = dict()\n",
    "for item in ingredients_list:\n",
    "    for ingredient in item:\n",
    "        if ingredient in ingredientDict:\n",
    "            ingredientDict[ingredient] +=1\n",
    "        else:\n",
    "            ingredientDict[ingredient] =1\n",
    "len(ingredientDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset consists of 39,774 rows of data, with 20 different categories of cuisines. However, we find that there is quite a bit of redundancy in the data, with only 6714 differnet unique ingredients. If we examine the ingredients even further, we see that the ingredients suffer from casing and phrasing issues. For example, there might be 'Romaine' and 'romaine', and phrases like \"Kraft Zesty Italian Dressing.\" So, go back and clean the ingredients so as to break apart any phrases into component words. 'Romaine Lettuce' becomes 'romaine', 'lettuce.' We also convert everything to lowercase, and drop any words that are 'a', 'the', 'of.' We then create a new dictionary from this and we find the number of unique ingredients to be about half as large at 3186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(matrix):\n",
    "    \"\"\" A function that accepts the matrix of ingredients as input. It loops through, splits any phrases into component\n",
    "        parts, and adds these to a new array. Also added to this array is the ingredient converted to lower case. \n",
    "        Anything that has 'a', 'the', 'and', 'of' is dropped. Then, we add this array to a new matrix. This returns\n",
    "        a matrix of lowercase, split, ingredients.\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    for array in matrix:\n",
    "        row = []\n",
    "        for ingr in array:\n",
    "            if ' ' in ingr:\n",
    "#                 print(ingr)\n",
    "                new_ingr_split = ingr.split()\n",
    "#                 print(new_ingr_split)\n",
    "                for item in new_ingr_split:\n",
    "                    if item != 'the' and ingr != 'of' and ingr != 'and' and ingr != 'a':\n",
    "                        row.append(item.lower())\n",
    "            else:\n",
    "                row.append(ingr.lower())\n",
    "        new_list.append(row)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_matrix = clean_data(ingredients_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## We Create a new dictionary from this cleaned data\n",
    "ingr_Dict = dict()\n",
    "for item in new_matrix:\n",
    "    for ingredient in item:\n",
    "        if ingredient in ingr_Dict:\n",
    "            ingr_Dict[ingredient] +=1\n",
    "        else:\n",
    "            ingr_Dict[ingredient] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3186"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the Number of unique ingredients in our cleaned data set\n",
    "len(ingr_Dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to one hot encode the ingredients. To do this, we first convert all of the ingredients to numerical representations. Then we represent each of these numbers a binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## first, we create a dictionary of each item in the dataset, and map each ingredient as a key to a numerical value.\n",
    "factorize_dict = dict()\n",
    "k=0\n",
    "for item in new_matrix:\n",
    "    for ingredient in item:\n",
    "        if ingredient not in factorize_dict:\n",
    "            k+=1\n",
    "            factorize_dict[ingredient] =k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3186"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The lengths of the dictionary with frequency counts and numerical encoudings are the same\n",
    "len(factorize_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now we go through the entire  matrix of ingredients, and replace each ingredient with the numerical value \n",
    "## In the dictionary we created above\n",
    "factorized_list = []\n",
    "for array in new_matrix:\n",
    "    row = []\n",
    "    for x in array:\n",
    "        num = factorize_dict[x]\n",
    "        row.append(num)\n",
    "    factorized_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "factorized_consolidated_list = list(factorize_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## next we create an empty numpy matrix of zeroes. We will use this later to set specific index locations to 1\n",
    "blank_data = np.empty((len(factorized_list), len(factorized_consolidated_list)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Now we loop through the entire factorized ingredient matrix represented with numerical values\n",
    "## and set that index location in the corresponding row to 1\n",
    "s = 0\n",
    "while s < len(factorized_list):\n",
    "    for item in factorized_list[s]:\n",
    "        blank_data[s][item] = 1\n",
    "    s+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##We also need to factorize the cuisines. Thankfully, pandas has a built in function\n",
    "df['cuisine'] = pd.factorize(df['cuisine'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Now, we set our X = to the one encoded matrix of ingredients,\n",
    "## and Y Values = to the cuisine labels represented as numbers\n",
    "X= blank_data\n",
    "Y = df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is:  (39774, 3187) Y shape is (39774,)\n"
     ]
    }
   ],
   "source": [
    "## We make sure the x and y shapes are the compatible\n",
    "print (\"X shape is: \", X.shape, \"Y shape is\" , Y.shape, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##make sure there are no nan values\n",
    "np.any(np.isnan(X)), np.any(np.isnan(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Next we check that our one hot encoding worked, and that we have 1 values in our empty matrix\n",
    "sum = 0\n",
    "for item in X:\n",
    "    for num in item: \n",
    "        sum = sum + num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752205.0\n"
     ]
    }
   ],
   "source": [
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26502153877819484"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "scores_gnb = cross_validation.cross_val_score(gnb, X, Y, cv=3)\n",
    "scores_gnb.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71317996873139122"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Bernouli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "scores_bnb = cross_validation.cross_val_score(bnb, X, Y, cv=3)\n",
    "scores_bnb.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at naive Bayes, there is a drastic difference in performance. Bernoulli NB almost performs as well as Logistic Regression with a perfomance of .71. Gaussian on the other hand performs with a very poor rate of .26. A possible explanation is that the Bernoulli distribution more closely matches the Yummly data set, and Gaussian distriutions do not do a particularly good job of modeling the data. Additionally, Bernoulli Naive Bayes by its very nature is designed to only deal with binary values. Since our data is one hot encoded, it is enginerred more for Bernoulli then Gaussian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78199308839376502"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validation.cross_val_score(LogisticRegression(), X, Y, cv=3)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see as expected that with 3 Fold Cross Validation, Logistic Regression performs the best with projected perfomance of .78."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
