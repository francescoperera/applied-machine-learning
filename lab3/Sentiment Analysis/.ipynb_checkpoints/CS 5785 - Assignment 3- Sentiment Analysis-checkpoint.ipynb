{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "from sklearn import decomposition\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Download and  load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#File Locations\n",
    "amazonFileName = \"sentiment labelled sentences/amazon_cells_labelled.txt\"\n",
    "yelpFileName = \"sentiment labelled sentences/yelp_labelled.txt\"\n",
    "imdbFileName = \"sentiment labelled sentences/imdb_labelled.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special case - imdb, both tabs and newlines need to be taken care of\n",
    "def cleanFile(fname):\n",
    "    f = open(fname,\"r\")\n",
    "    cleanOutput=[]\n",
    "    for line in f:\n",
    "        line = line.replace(\"\\n\",\"\")\n",
    "        cleanOutput.append(line.split(\"\\t\"))\n",
    "    return cleanOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import Data into Pandas\n",
    "amazon = pd.read_csv(amazonFileName, sep=\"\\t\",header=None).dropna().reset_index().drop(\"index\",axis =1)\n",
    "yelp = pd.read_csv(yelpFileName, sep=\"\\t\", header=None,encoding=\"utf-8\").dropna().reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "cleanImdb = cleanFile(imdbFileName)\n",
    "imdb = pd.DataFrame(cleanImdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     0  1\n",
      "0    So there is no way for me to plug it in here i...  0\n",
      "1                          Good case, Excellent value.  1\n",
      "2                               Great for the jawbone.  1\n",
      "3    Tied to charger for conversations lasting more...  0\n",
      "4                                    The mic is great.  1\n",
      "5    I have to jiggle the plug to get it to line up...  0\n",
      "6    If you have several dozen or several hundred c...  0\n",
      "7          If you are Razr owner...you must have this!  1\n",
      "8                  Needless to say, I wasted my money.  0\n",
      "9                     What a waste of money and time!.  0\n",
      "10                     And the sound quality is great.  1\n",
      "11   He was very impressed when going from the orig...  1\n",
      "12   If the two were seperated by a mere 5+ ft I st...  0\n",
      "13                            Very good quality though  1\n",
      "14   The design is very odd, as the ear \"clip\" is n...  0\n",
      "15   Highly recommend for any one who has a blue to...  1\n",
      "16                 I advise EVERYONE DO NOT BE FOOLED!  0\n",
      "17                                    So Far So Good!.  1\n",
      "18                                       Works great!.  1\n",
      "19   It clicks into place in a way that makes you w...  0\n",
      "20   I went on Motorola's website and followed all ...  0\n",
      "21   I bought this to use with my Kindle Fire and a...  1\n",
      "22            The commercials are the most misleading.  0\n",
      "23   I have yet to run this new battery below two b...  1\n",
      "24   I bought it for my mother and she had a proble...  0\n",
      "25                Great Pocket PC / phone combination.  1\n",
      "26   I've owned this phone for 7 months now and can...  1\n",
      "27   I didn't think that the instructions provided ...  0\n",
      "28   People couldnt hear me talk and I had to pull ...  0\n",
      "29                                Doesn't hold charge.  0\n",
      "..                                                 ... ..\n",
      "970  I plugged it in only to find out not a darn th...  0\n",
      "971                                 Excellent product.  1\n",
      "972                        Earbud piece breaks easily.  0\n",
      "973                                     Lousy product.  0\n",
      "974  This phone tries very hard to do everything bu...  0\n",
      "975  It is the best charger I have seen on the mark...  1\n",
      "976                                  SWEETEST PHONE!!!  1\n",
      "977             :-)Oh, the charger seems to work fine.  1\n",
      "978  It fits so securely that the ear hook does not...  1\n",
      "979                                 Not enough volume.  0\n",
      "980                Echo Problem....Very unsatisfactory  0\n",
      "981  you could only take 2 videos at a time and the...  0\n",
      "982                            don't waste your money.  0\n",
      "983  I am going to have to be the first to negative...  0\n",
      "984  Adapter does not provide enough charging current.  0\n",
      "985  There was so much hype over this phone that I ...  0\n",
      "986  You also cannot take pictures with it in the c...  0\n",
      "987                            Phone falls out easily.  0\n",
      "988  It didn't work, people can not hear me when I ...  0\n",
      "989  The text messaging feature is really tricky to...  0\n",
      "990  I'm really disappointed all I have now is a ch...  0\n",
      "991                                Painful on the ear.  0\n",
      "992                   Lasted one day and then blew up.  0\n",
      "993                                      disappointed.  0\n",
      "994                              Kind of flops around.  0\n",
      "995  The screen does get smudged easily because it ...  0\n",
      "996  What a piece of junk.. I lose more calls on th...  0\n",
      "997                       Item Does Not Match Picture.  0\n",
      "998  The only thing that disappoint me is the infra...  0\n",
      "999  You can not answer calls with the unit, never ...  0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b Preprocessing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lowerCase(x):\n",
    "    \"\"\"\n",
    "    x is a string. Every letter in x is turned to lowercase\n",
    "    \"\"\"\n",
    "    return x.lower()\n",
    "\n",
    "def lemmatize(lmt,x):\n",
    "    \"\"\"\n",
    "    It takes a word x and the lemmatizer lmt from nltk. Lmt lemmatizes x\n",
    "    \"\"\"\n",
    "    return lmt.lemmatize(x)\n",
    "\n",
    "def stripPunctuation(s):\n",
    "    \"\"\"\n",
    "    It takes a string x and uses regular expression library to remove punctuation from it.\n",
    "    Reference: https://www.quora.com/How-do-I-remove-punctuation-from-a-Python-string\n",
    "    \"\"\"\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    return regex.sub(' ', s)\n",
    "\n",
    "\n",
    "def getStopWords():\n",
    "    # Do all stop words need to be excluded. Should we keep negatives like no or not? - Francesco 10/29.\n",
    "    return set(stopwords.words(\"english\")) \n",
    "\n",
    "def preProcessing(x):\n",
    "    processedString = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopWords = getStopWords()\n",
    "    xLowerCase = lowerCase(x)\n",
    "    xWithoutPunct = stripPunctuation(xLowerCase)\n",
    "    \n",
    "    for word in xWithoutPunct.split():\n",
    "        try:\n",
    "            if unicode(word) not in stopWords:\n",
    "                lemmatizedWord = lemmatize(lemmatizer, unicode(word))\n",
    "                processedString.append(str(lemmatizedWord))\n",
    "        except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "            print \"UnicodeDecodeError: \" + word\n",
    "       \n",
    "    return processedString    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnicodeDecodeError: fiancé\n",
      "UnicodeDecodeError: café\n",
      "UnicodeDecodeError: crêpe\n",
      "UnicodeDecodeError: puréed\n",
      "UnicodeDecodeError: \n",
      "UnicodeDecodeError: québec\n",
      "UnicodeDecodeError: is",
      "was\n",
      "UnicodeDecodeError: \n",
      "UnicodeDecodeError: clichés\n",
      "UnicodeDecodeError: clichés\n",
      "UnicodeDecodeError: aurvåg\n",
      "UnicodeDecodeError: \n",
      "UnicodeDecodeError: problemsthe\n",
      "UnicodeDecodeError: \n",
      "UnicodeDecodeError: clichés\n",
      "UnicodeDecodeError: \n",
      "UnicodeDecodeError: seeing",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Apply the preprocessing method on the sentences in the amazon,yelp and imdb dataframes.\n",
    "\"\"\"\n",
    "amazon[0] = amazon[0].map(preProcessing)\n",
    "yelp[0] = yelp[0].map(preProcessing)\n",
    "imdb[0] = imdb[0].map(preProcessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The label values in the imdb dataframe need to casted to ints\n",
    "\"\"\"\n",
    "imdb[1] = imdb[1].map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c Split training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazonPositiveTrain = amazon[amazon[1] == 1].head(400)\n",
    "amazonPositiveTest = amazon[amazon[1] == 1].head(100)\n",
    "amazonNegativeTrain = amazon[amazon[1] == 0].head(400)\n",
    "amazonNegativeTest = amazon[amazon[1] == 0].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelpPositiveTrain = yelp[yelp[1] == 1].head(400)\n",
    "yelpPositiveTest = yelp[yelp[1] == 1].head(100)\n",
    "yelpNegativeTrain = yelp[yelp[1] == 0].head(400)\n",
    "yelpNegativeTest = yelp[yelp[1] == 0].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdbPositiveTrain = imdb[imdb[1] == 1].head(400)\n",
    "imdbPositiveTest = imdb[imdb[1] == 1].head(100)\n",
    "imdbNegativeTrain = imdb[imdb[1] == 0].head(400)\n",
    "imdbNegativeTest = imdb[imdb[1] == 0].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainFrames = [amazonPositiveTrain,amazonNegativeTrain,\n",
    "               yelpPositiveTrain,yelpNegativeTrain,\n",
    "               imdbPositiveTrain,imdbNegativeTrain]\n",
    "\n",
    "testFrames = [amazonPositiveTest,amazonNegativeTest,\n",
    "              yelpPositiveTest,yelpNegativeTest,\n",
    "              imdbPositiveTest,imdbNegativeTest]\n",
    "\n",
    "trainDF = pd.concat(trainFrames).reset_index().drop(\"index\",axis=1)\n",
    "testDF =  pd.concat(testFrames).reset_index().drop(\"index\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 2)\n",
      "(600, 2)\n"
     ]
    }
   ],
   "source": [
    "print trainDF.shape\n",
    "print testDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.d Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateBag(reviews):\n",
    "    \"\"\"\n",
    "    words is a list of words. The function creates a dictionary of unique words\n",
    "    \"\"\"\n",
    "    wordBag ={}\n",
    "    for review in reviews:\n",
    "        for word in review:\n",
    "            if word not in wordBag.keys():\n",
    "                wordBag[word] = 0\n",
    "    return wordBag\n",
    "\n",
    "def indexBag(bag):\n",
    "    \"\"\"\n",
    "    bag is the input. The function essentially assigns an index for each word in the bag.The index will be needed\n",
    "    for counting the frequency of its word and create the correct feature vector.\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    for word in bag.keys():\n",
    "        bag[word] = idx\n",
    "        idx+=1\n",
    "    return bag\n",
    "\n",
    "def wordFrequency(reviews,bag):\n",
    "    \"\"\"\n",
    "    iterates through a review and using the bag, it returns the feature vectors of all the reviews.\n",
    "    \"\"\"\n",
    "    featureVectorList = []\n",
    "    for review in reviews:\n",
    "        featureVec = [0.0 for uniqueWord in range(0,len(bag.keys()))]\n",
    "        for word in review:\n",
    "            if word in bag.keys():\n",
    "                #update the value in featureVec whose index is found in bag[word]\n",
    "                featureVec[bag[word]]+=1.0\n",
    "            else:\n",
    "                print \"Error: \" + word + \" is not in the bag.\"\n",
    "        featureVectorList.append(featureVec)\n",
    "    return pd.DataFrame(featureVectorList,columns = bag.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate bag only from Training Data\n",
    "bagOfWords = generateBag(trainDF[0].tolist())\n",
    "indexedBag = indexBag(bagOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate all the feature vectors for Training and Test Data\n",
    "trainingVecs = wordFrequency(trainDF[0].tolist(),indexedBag)\n",
    "testVecs = wordFrequency(testDF[0].tolist(),indexedBag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   limited  versatile  secondly  magnetic  personally  bear  yellow  sleek  \\\n",
      "0        0          0         0         0           0     0       0      0   \n",
      "1        0          0         0         0           0     0       0      0   \n",
      "\n",
      "   four  sleep   ...     jewel  forgetting  portion  pandering  compete  \\\n",
      "0     0      0   ...         0           0        0          0        0   \n",
      "1     0      0   ...         0           0        0          0        0   \n",
      "\n",
      "   monstrous  searched  yell  abhor  jawbone  \n",
      "0          0         0     0      0        0  \n",
      "1          0         0     0      0        1  \n",
      "\n",
      "[2 rows x 4075 columns]\n"
     ]
    }
   ],
   "source": [
    "#Report feature vectors any two reviews\n",
    "print trainingVecs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.e Postprocessing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### L1 or lasso was chosen as the normalization method because it best suits sparse data.\n",
    "def normalize(df):\n",
    "    normalizedDf = preprocessing.normalize(df, norm='l1')\n",
    "    return pd.DataFrame(normalizedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalizedTraining = normalize(trainingVecs)\n",
    "normalizedTest = normalize(testVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.f Training set clustering (K-means implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFirstMeans(trainData,k):\n",
    "    firstMeans = []\n",
    "    idxs = [x for x in range(len(trainData))]\n",
    "    meanIdxs = random.sample(idxs,k)\n",
    "    for idx in meanIdxs:\n",
    "        firstMeans.append(trainData.iloc[idx].values)\n",
    "    return firstMeans\n",
    "\n",
    "def clustering(trainData,clusterMeans,k):\n",
    "    rowIdxs = [i for i in range(len(trainData))]\n",
    "    clusters = []\n",
    "    for num in range(k):\n",
    "        clusters.append([])\n",
    "    \n",
    "    for row in rowIdxs :\n",
    "        distances = []\n",
    "        for mean in clusterMeans:\n",
    "            dist = np.linalg.norm(trainData.iloc[row] - mean)\n",
    "            distances.append(dist)\n",
    "        clusterIdx = np.argmin(distances)\n",
    "        \n",
    "        clusters[clusterIdx].append(row)\n",
    "    return clusters\n",
    "    \n",
    "def newMeans(trainData,clusters):\n",
    "    means = []\n",
    "    for cluster in clusters:\n",
    "        clusterMean = np.mean(trainData.iloc[cluster])\n",
    "        means.append(clusterMean)\n",
    "    return means\n",
    "\n",
    "def KMeans(trainData,k):\n",
    "    iters = 0\n",
    "    initMeans = getFirstMeans(trainData,k)\n",
    "    clusters1 = clustering(trainData,initMeans,k)\n",
    "    means1 =  newMeans(trainData,clusters1)\n",
    "    clusters2 = clustering(trainData,means1,k)\n",
    "    means2 = newMeans(trainData,clusters2)\n",
    "    print len(means1),len(means2)\n",
    "    while clusters1 != clusters2:\n",
    "        print \"iteration: \" + str(iters+1)\n",
    "        clusters1 = clustering(trainData,means2,k)\n",
    "        means1 = newMeans(trainData,clusters1)\n",
    "        clusters2 = clustering(trainData,means1,k)\n",
    "        means2 = newMeans(trainData,clusters2)\n",
    "        print len(means1),len(means2)\n",
    "        iters += 1\n",
    "    \n",
    "    return clusters2,means2\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "iteration: 1\n",
      "2 2\n",
      "iteration: 2\n",
      "2 2\n",
      "iteration: 3\n",
      "2 2\n",
      "iteration: 4\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "kMeansCluster,kMeansClusterMean= KMeans(normalizedTraining,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0       0.000000\n",
      "1       0.000000\n",
      "2       0.000000\n",
      "3       0.000000\n",
      "4       0.000000\n",
      "5       0.000000\n",
      "6       0.000000\n",
      "7       0.000000\n",
      "8       0.000000\n",
      "9       0.000000\n",
      "10      0.000000\n",
      "11      0.000000\n",
      "12      0.000000\n",
      "13      0.000000\n",
      "14      0.000000\n",
      "15      0.000000\n",
      "16      0.000000\n",
      "17      0.000000\n",
      "18      0.000000\n",
      "19      0.000000\n",
      "20      0.000000\n",
      "21      0.000000\n",
      "22      0.000000\n",
      "23      0.000000\n",
      "24      0.000000\n",
      "25      0.000000\n",
      "26      0.000000\n",
      "27      0.000000\n",
      "28      0.000000\n",
      "29      0.000000\n",
      "          ...   \n",
      "4045    0.000000\n",
      "4046    0.000000\n",
      "4047    0.000000\n",
      "4048    0.000000\n",
      "4049    0.000000\n",
      "4050    0.000000\n",
      "4051    0.004167\n",
      "4052    0.000000\n",
      "4053    0.000000\n",
      "4054    0.000000\n",
      "4055    0.000000\n",
      "4056    0.000000\n",
      "4057    0.000000\n",
      "4058    0.000000\n",
      "4059    0.000000\n",
      "4060    0.000000\n",
      "4061    0.000000\n",
      "4062    0.000000\n",
      "4063    0.000000\n",
      "4064    0.000000\n",
      "4065    0.000000\n",
      "4066    0.000000\n",
      "4067    0.000000\n",
      "4068    0.000000\n",
      "4069    0.000000\n",
      "4070    0.000000\n",
      "4071    0.000000\n",
      "4072    0.000000\n",
      "4073    0.000000\n",
      "4074    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print len(kMeansCluster)\n",
    "#print kMeansClusterMean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def displayResults(dfTrain,clusters):\n",
    "    dfTrain[1] = dfTrain[1].map(int)\n",
    "    for i,cluster in enumerate(clusters):\n",
    "        clusterData = dfTrain.iloc[cluster]\n",
    "        clusterData[1] = clusterData[1].map(int)\n",
    "        print\n",
    "        print \"Number of points in cluster \" + str(i) + \" :\"  + str(len(cluster))\n",
    "        print \"Number of 1s in cluster \" + str(i) + \" :\" + str(clusterData[clusterData[1] == 1].shape[0])\n",
    "        print \"percent of 1s in  cluster \" + str(i) + \" :\" + str((clusterData[clusterData[1] == 1].shape[0])/float(len(cluster))) \n",
    "        print \"Number of 0s in cluster \" + str(i) + \" :\" + str(clusterData[clusterData[1] == 0].shape[0])\n",
    "        print \"percent of 0s in cluster \" +  str(i) + \" :\" + str((clusterData[clusterData[1] == 0].shape[0])/float(len(cluster)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of points in cluster 0 :48\n",
      "Number of 1s in cluster 0 :22\n",
      "percent of 1s in  cluster 0 :0.458333333333\n",
      "Number of 0s in cluster 0 :26\n",
      "percent of 0s in cluster 0 :0.541666666667\n",
      "\n",
      "Number of points in cluster 1 :2352\n",
      "Number of 1s in cluster 1 :1178\n",
      "percent of 1s in  cluster 1 :0.500850340136\n",
      "Number of 0s in cluster 1 :1174\n",
      "percent of 0s in cluster 1 :0.499149659864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescoperera/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "displayResults(trainDF,kMeansCluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it also for  testDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.g Sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logisticRegression(trainVecs,trainLabels,testVecs,testLabels):\n",
    "    model = linear_model.LogisticRegression()\n",
    "    modelFit = model.fit(trainVecs, trainLabels)\n",
    "    score = modelFit.score(testVecs, testLabels)\n",
    "    pred = modelFit.predict(testVecs)\n",
    "    confusionMatrix = confusion_matrix(pred, testLabels)\n",
    "    return model,score,confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model,score,confusionMatrix = logisticRegression(normalizedTraining,trainDF[1],normalizedTest,testDF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843333333333\n",
      "[[270  64]\n",
      " [ 30 236]]\n"
     ]
    }
   ],
   "source": [
    "print score\n",
    "print confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getInfluentialWords(m,vecList):\n",
    "    print len(m)\n",
    "    negIdx, negVal = max(enumerate(m[0]), key=operator.itemgetter(1))\n",
    "    posIdx,posVal = max(enumerate(m[1]), key=operator.itemgetter(1))\n",
    "    print negIdx, negVal\n",
    "    print posIdx,posVal\n",
    "    negWord = vecList[0][negIdx] # row index does not matter, we are looking for the columns\n",
    "    posWord = vecList[0][posIdx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-587ecc76d456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetInfluentialWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainingVecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-383f06dc509d>\u001b[0m in \u001b[0;36mgetInfluentialWords\u001b[0;34m(m, vecList)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnegIdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mposIdx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mnegIdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegVal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mposIdx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposVal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "getInfluentialWords(model.coef_,trainingVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.h N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateBiGramDict(reviews):\n",
    "    \"\"\"\n",
    "    words is a list of words. The function creates a dictionary of unique words\n",
    "    \"\"\"\n",
    "    biGramDict ={}\n",
    "    for review in reviews:\n",
    "        for i in range(len(review)-1):\n",
    "            biGram= review[i] + \" \" + review[i+1]\n",
    "            #biGram = str(review)\n",
    "            if biGram not in  biGramDict.keys():\n",
    "                biGramDict[biGram] = 0\n",
    "    return biGramDict\n",
    "\n",
    "def indexBiGram(d):\n",
    "    \"\"\"\n",
    "    bag is the input. The function essentially assigns an index for each word in the bag.The index will be needed\n",
    "    for counting the frequency of its word and create the correct feature vector.\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    for biGram in d.keys():\n",
    "        d[biGram] = idx\n",
    "        idx+=1\n",
    "    return d\n",
    "\n",
    "def biGramFrequency(reviews,d):\n",
    "    \"\"\"\n",
    "    iterates through a review and using the bag, it returns the feature vector of the review.\n",
    "    \"\"\"\n",
    "    featureVectorList = []\n",
    "    for review in reviews:\n",
    "        featureVec = [0.0 for x in range(0,len(d.keys()))]\n",
    "        for i in range(len(review)-1):\n",
    "            biGram= review[i] + \" \" + review[i+1]\n",
    "            if biGram in d.keys():\n",
    "                #update the value in featureVec whose index is found in bag[word]\n",
    "                featureVec[d[biGram]]+=1.0\n",
    "            else:\n",
    "                print \"Error: \" + biGram + \" not in dictionary.\"\n",
    "        featureVectorList.append(featureVec)\n",
    "    return pd.DataFrame(featureVectorList,columns = d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate bag only from Training Data\n",
    "biGramDict = generateBiGramDict(trainDF[0].tolist())\n",
    "indexedBiGram = indexBiGram(biGramDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate all the feature vectors for Training and Test Data\n",
    "biGramTrainingVecs = biGramFrequency(trainDF[0].tolist(),indexedBiGram)\n",
    "biGramTestVecs = biGramFrequency(testDF[0].tolist(),indexedBiGram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Report feature vectors any two reviews\n",
    "print biGramTrainingVecs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Logistic Regression on the normalized train and test vectors obtained from the Bi Gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizedBiGramTrainingVecs = normalize(biGramTrainingVecs)\n",
    "normalizedBiGramTestVecs = normalize(biGramTestVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biGramScore,biGramConfusionMatrix = logisticRegression(normalizedBiGramTrainingVecs,trainDF[1],\n",
    "                                                       normalizedBiGramTestVecs,testDF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print biGramScore\n",
    "print biGramConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.i PCA for bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PCA Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Center the data by subtracting the mean from it.\n",
    "2. Calculate the covariance matrix.\n",
    "3. Calculate the eigenvectors of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center(x,mean):\n",
    "    for row in x:\n",
    "        row -= mean\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingMean = normalizedTraining.mean()\n",
    "testMean = normalizedTest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centeredTraining = center(normalizedTraining,trainingMean)\n",
    "centeredTest = center(normalizedTest,testMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute SVD to get the eigenvectors and the eigenvalues\n",
    "UTrain,DTrain, VTrain = scipy.linalg.svd(centeredTraining, full_matrices=False)\n",
    "UTest, DTest, VTest = scipy.linalg.svd(centeredTest, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print UTrain.shape,DTrain.shape, VTrain.shape\n",
    "print UTest.shape, DTest.shape, VTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PCA(U,D,V,x):\n",
    "    \"\"\"\n",
    "    takes the SVD decomposition matrices and returns a lower dimensional representation of the data\n",
    "    \"\"\"\n",
    "    size = D.shape[0]\n",
    "    newD = np.zeros((size,size))\n",
    "    newDiagonal = np.concatenate((D[:x],[0] * (len(D) - x)))\n",
    "    i  = 0\n",
    "    j = 0\n",
    "    while i < size and j< size:\n",
    "        newD[i][j] = newDiagonal[i]\n",
    "        i+=1\n",
    "        j+=1\n",
    "    return pd.DataFrame(np.dot(U, np.dot(newD,V)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train10 = PCA(UTrain,DTrain,VTrain,10)\n",
    "train50 = PCA(UTrain,DTrain,VTrain,50)\n",
    "train100 = PCA(UTrain,DTrain,VTrain,100)\n",
    "\n",
    "test10 = PCA(UTest,DTest,VTest,10)\n",
    "test50 = PCA(UTest,DTest,VTest,50)\n",
    "test100 = PCA(UTest,DTest,VTest,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizedTrain10 = normalize(train10)\n",
    "normalizedTest10 = normalize(test10)\n",
    "\n",
    "normalizedTrain50 = normalize(train50)\n",
    "normalizedTest50 = normalize(test50)\n",
    "\n",
    "normalizedTrain100 = normalize(train100)\n",
    "normalizedTest100 = normalize(test100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score10,confusionMatrix10 = logisticRegression(normalizedTrain10,trainDF[1],normalizedTest10,testDF[1])\n",
    "score50,confusionMatrix50 = logisticRegression(normalizedTrain50,trainDF[1],normalizedTest50,testDF[1])\n",
    "score100,confusionMatrix100 = logisticRegression(normalizedTrain100,trainDF[1],normalizedTest100,testDF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print score10,score50,score100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print confusionMatrix10\n",
    "print confusionMatrix50\n",
    "print confusionMatrix100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truePCA(x,n):\n",
    "    pca = decomposition.PCA(n)\n",
    "    pca.fit(x)\n",
    "    newX = pca.transform(x)\n",
    "    return newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr10 = truePCA(normalizedTraining,10)\n",
    "tr50 = truePCA(normalizedTraining,50)\n",
    "tr100 = truePCA(normalizedTraining,100)\n",
    "\n",
    "te10 = truePCA(normalizedTest,10)\n",
    "te50 = truePCA(normalizedTest,50)\n",
    "te100 = truePCA(normalizedTest,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntr10 = normalize(tr10)\n",
    "ntr50 = normalize(tr50)\n",
    "ntr100 = normalize(tr100)\n",
    "\n",
    "nte10 = normalize(te10)\n",
    "nte50 = normalize(te50)\n",
    "nte100 = normalize(te100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sklearnScore10,sklearnConfusionMatrix10 = logisticRegression(ntr10,trainDF[1],nte10,testDF[1])\n",
    "sklearnScore50,sklearnConfusionMatrix50 = logisticRegression(ntr50,trainDF[1],nte50,testDF[1])\n",
    "sklearnScore100,sklearnConfusionMatrix100 = logisticRegression(ntr100,trainDF[1],nte100,testDF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sklearnScore10,sklearnScore50,sklearnScore100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
