{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "from sklearn import decomposition\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Download and  load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#File Locations\n",
    "amazonFileName = \"sentiment labelled sentences/amazon_cells_labelled.txt\"\n",
    "yelpFileName = \"sentiment labelled sentences/yelp_labelled.txt\"\n",
    "imdbFileName = \"sentiment labelled sentences/imdb_labelled.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special case - imdb, both tabs and newlines need to be taken care of\n",
    "def cleanFile(fname):\n",
    "    f = open(fname,\"r\")\n",
    "    cleanOutput=[]\n",
    "    for line in f:\n",
    "        line = line.replace(\"\\n\",\"\")\n",
    "        cleanOutput.append(line.split(\"\\t\"))\n",
    "    return cleanOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import Data into Pandas\n",
    "amazon = pd.read_csv(amazonFileName, sep=\"\\t\",header=None).dropna().reset_index().drop(\"index\",axis =1)\n",
    "yelp = pd.read_csv(yelpFileName, sep=\"\\t\", header=None,encoding=\"utf-8\").dropna().reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "cleanImdb = cleanFile(imdbFileName)\n",
    "imdb = pd.DataFrame(cleanImdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getLabelRatio(df):\n",
    "    df[1] = df[1].map(int)\n",
    "    reviews = df[df[1] == 0]\n",
    "    zeroRatio = reviews.shape[0]/ float(df.shape[0])\n",
    "    print \"The file contains \" + str(zeroRatio) + \"% of 0s and \" + str(1 - zeroRatio) + \"% of 1s.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 0.5% of 0s and 0.5% of 1s.\n",
      "The file contains 0.5% of 0s and 0.5% of 1s.\n",
      "The file contains 0.5% of 0s and 0.5% of 1s.\n"
     ]
    }
   ],
   "source": [
    "getLabelRatio(amazon)\n",
    "getLabelRatio(yelp)\n",
    "getLabelRatio(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     0  1\n",
      "0    A very, very, very slow-moving, aimless movie ...  0\n",
      "1    Not sure who was more lost - the flat characte...  0\n",
      "2    Attempting artiness with black & white and cle...  0\n",
      "3         Very little music or anything to speak of.    0\n",
      "4    The best scene in the movie was when Gerardo i...  1\n",
      "5    The rest of the movie lacks art, charm, meanin...  0\n",
      "6                                  Wasted two hours.    0\n",
      "7    Saw the movie today and thought it was a good ...  1\n",
      "8                                 A bit predictable.    0\n",
      "9    Loved the casting of Jimmy Buffet as the scien...  1\n",
      "10                And those baby owls were adorable.    1\n",
      "11   The movie showed a lot of Florida at it's best...  1\n",
      "12   The Songs Were The Best And The Muppets Were S...  1\n",
      "13                                   It Was So Cool.    1\n",
      "14   This is a very \"right on case\" movie that deli...  1\n",
      "15   It had some average acting from the main perso...  0\n",
      "16   This review is long overdue, since I consider ...  1\n",
      "17   I'll put this gem up against any movie in term...  1\n",
      "18   It's practically perfect in all of them  a tr...  1\n",
      "19   \" The structure of this film is easily the mos...  1\n",
      "20   I can think of no other film where something v...  1\n",
      "21   In other words, the content level of this film...  1\n",
      "22   How can anyone in their right mind ask for any...  1\n",
      "23   It's quite simply the highest, most superlativ...  1\n",
      "24   Yes, this film does require a rather significa...  1\n",
      "25       This short film certainly pulls no punches.    0\n",
      "26   Graphics is far from the best part of the game.    0\n",
      "27   This is the number one best TH game in the ser...  1\n",
      "28                          It deserves strong love.    1\n",
      "29                             It is an insane game.    1\n",
      "..                                                 ... ..\n",
      "970  Enough can not be said of the remarkable anima...  1\n",
      "971  The art style has the appearance of crayon/pen...  1\n",
      "972  If you act in such a film, you should be glad ...  0\n",
      "973  This one wants to surf on the small wave of sp...  0\n",
      "974  If you haven't choked in your own vomit by the...  0\n",
      "975  Still, it makes up for all of this with a supe...  1\n",
      "976  Just consider the excellent story, solid actin...  1\n",
      "977  Instead, we got a bore fest about a whiny, spo...  0\n",
      "978  Then I watched it again two Sundays ago (March...  1\n",
      "979       It is a very well acted and done TV Movie.    1\n",
      "980  Judith Light is one of my favorite actresses a...  1\n",
      "981                I keep watching it over and over.    1\n",
      "982                 It's a sad movie, but very good.    1\n",
      "983  If you have not seen this movie, I definitely ...  1\n",
      "984           She is as lovely as usual, this cutie!    1\n",
      "985  Still it's quite interesting and entertaining ...  1\n",
      "986                    ;) Recommend with confidence!    1\n",
      "987  This movie is well-balanced with comedy and dr...  1\n",
      "988  It was a riot to see Hugo Weaving play a sex-o...  1\n",
      "989  :) Anyway, the plot flowed smoothly and the ma...  1\n",
      "990  The opening sequence of this gem is a classic,...  1\n",
      "991             Fans of the genre will be in heaven.    1\n",
      "992                Lange had become a great actress.    1\n",
      "993                It looked like a wonderful story.    1\n",
      "994            I never walked out of a movie faster.    0\n",
      "995  I just got bored watching Jessice Lange take h...  0\n",
      "996  Unfortunately, any virtue in this film's produ...  0\n",
      "997                   In a word, it is embarrassing.    0\n",
      "998                               Exceptionally bad!    0\n",
      "999  All in all its an insult to one's intelligence...  0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b Preprocessing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lowerCase(x):\n",
    "    \"\"\"\n",
    "    x is a string. Every letter in x is turned to lowercase\n",
    "    \"\"\"\n",
    "    return x.lower()\n",
    "\n",
    "def lemmatize(lmt,x):\n",
    "    \"\"\"\n",
    "    It takes a word x and the lemmatizer lmt from nltk. Lmt lemmatizes x\n",
    "    \"\"\"\n",
    "    return lmt.lemmatize(x)\n",
    "\n",
    "def stripPunctuation(s):\n",
    "    \"\"\"\n",
    "    It takes a string x and uses regular expression library to remove punctuation from it.\n",
    "    Reference: https://www.quora.com/How-do-I-remove-punctuation-from-a-Python-string\n",
    "    \"\"\"\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    return regex.sub(' ', s)\n",
    "\n",
    "\n",
    "def getStopWords():\n",
    "    # Do all stop words need to be excluded. Should we keep negatives like no or not? - Francesco 10/29.\n",
    "    return set(stopwords.words(\"english\")) \n",
    "\n",
    "def preProcessing(x):\n",
    "    processedString = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopWords = getStopWords()\n",
    "    xLowerCase = lowerCase(x)\n",
    "    xWithoutPunct = stripPunctuation(xLowerCase)\n",
    "    \n",
    "    for word in xWithoutPunct.split():\n",
    "        try:\n",
    "            if unicode(word) not in stopWords:\n",
    "                lemmatizedWord = lemmatize(lemmatizer, unicode(word))\n",
    "                processedString.append(str(lemmatizedWord))\n",
    "        except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "            print \"UnicodeDecodeError: \" + word\n",
    "       \n",
    "    return processedString    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnicodeDecodeError: fiancé\n",
      "UnicodeDecodeError: café\n",
      "UnicodeDecodeError: crêpe\n",
      "UnicodeDecodeError: puréed\n",
      "UnicodeDecodeError: \n",
      "UnicodeDecodeError: québec\n",
      "UnicodeDecodeError: is",
      "was\n",
      "UnicodeDecodeError: \n",
      "UnicodeDecodeError: clichés\n",
      "UnicodeDecodeError: clichés\n",
      "UnicodeDecodeError: aurvåg\n",
      "UnicodeDecodeError: \n",
      "UnicodeDecodeError: problemsthe\n",
      "UnicodeDecodeError: \n",
      "UnicodeDecodeError: clichés\n",
      "UnicodeDecodeError: \n",
      "UnicodeDecodeError: seeing",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Apply the preprocessing method on the sentences in the amazon,yelp and imdb dataframes.\n",
    "\"\"\"\n",
    "amazon[0] = amazon[0].map(preProcessing)\n",
    "yelp[0] = yelp[0].map(preProcessing)\n",
    "imdb[0] = imdb[0].map(preProcessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The label values in the imdb dataframe need to casted to ints\n",
    "\"\"\"\n",
    "imdb[1] = imdb[1].map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c Split training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazonPositiveTrain = amazon[amazon[1] == 1].head(400)\n",
    "amazonPositiveTest = amazon[amazon[1] == 1].head(100)\n",
    "amazonNegativeTrain = amazon[amazon[1] == 0].head(400)\n",
    "amazonNegativeTest = amazon[amazon[1] == 0].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelpPositiveTrain = yelp[yelp[1] == 1].head(400)\n",
    "yelpPositiveTest = yelp[yelp[1] == 1].head(100)\n",
    "yelpNegativeTrain = yelp[yelp[1] == 0].head(400)\n",
    "yelpNegativeTest = yelp[yelp[1] == 0].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdbPositiveTrain = imdb[imdb[1] == 1].head(400)\n",
    "imdbPositiveTest = imdb[imdb[1] == 1].head(100)\n",
    "imdbNegativeTrain = imdb[imdb[1] == 0].head(400)\n",
    "imdbNegativeTest = imdb[imdb[1] == 0].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainFrames = [amazonPositiveTrain,amazonNegativeTrain,\n",
    "               yelpPositiveTrain,yelpNegativeTrain,\n",
    "               imdbPositiveTrain,imdbNegativeTrain]\n",
    "\n",
    "testFrames = [amazonPositiveTest,amazonNegativeTest,\n",
    "              yelpPositiveTest,yelpNegativeTest,\n",
    "              imdbPositiveTest,imdbNegativeTest]\n",
    "\n",
    "trainDF = pd.concat(trainFrames).reset_index().drop(\"index\",axis=1)\n",
    "testDF =  pd.concat(testFrames).reset_index().drop(\"index\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 2)\n",
      "(600, 2)\n"
     ]
    }
   ],
   "source": [
    "print trainDF.shape\n",
    "print testDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.d Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateBag(reviews):\n",
    "    \"\"\"\n",
    "    words is a list of words. The function creates a dictionary of unique words\n",
    "    \"\"\"\n",
    "    wordBag ={}\n",
    "    for review in reviews:\n",
    "        for word in review:\n",
    "            if word not in wordBag.keys():\n",
    "                wordBag[word] = 0\n",
    "    return wordBag\n",
    "\n",
    "def indexBag(bag):\n",
    "    \"\"\"\n",
    "    bag is the input. The function essentially assigns an index for each word in the bag.The index will be needed\n",
    "    for counting the frequency of its word and create the correct feature vector.\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    for word in bag.keys():\n",
    "        bag[word] = idx\n",
    "        idx+=1\n",
    "    return bag\n",
    "\n",
    "def wordFrequency(reviews,bag):\n",
    "    \"\"\"\n",
    "    iterates through a review and using the bag, it returns the feature vectors of all the reviews.\n",
    "    \"\"\"\n",
    "    featureVectorList = []\n",
    "    for review in reviews:\n",
    "        featureVec = [0.0 for uniqueWord in range(0,len(bag.keys()))]\n",
    "        for word in review:\n",
    "            if word in bag.keys():\n",
    "                #update the value in featureVec whose index is found in bag[word]\n",
    "                featureVec[bag[word]]+=1.0\n",
    "            else:\n",
    "                print \"Error: \" + word + \" is not in the bag.\"\n",
    "        featureVectorList.append(featureVec)\n",
    "    return pd.DataFrame(featureVectorList,columns = bag.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate bag only from Training Data\n",
    "bagOfWords = generateBag(trainDF[0].tolist())\n",
    "indexedBag = indexBag(bagOfWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate all the feature vectors for Training and Test Data\n",
    "trainingVecs = wordFrequency(trainDF[0].tolist(),indexedBag)\n",
    "testVecs = wordFrequency(testDF[0].tolist(),indexedBag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   limited  versatile  secondly  magnetic  personally  bear  yellow  sleek  \\\n",
      "0        0          0         0         0           0     0       0      0   \n",
      "1        0          0         0         0           0     0       0      0   \n",
      "\n",
      "   four  sleep   ...     jewel  forgetting  portion  pandering  compete  \\\n",
      "0     0      0   ...         0           0        0          0        0   \n",
      "1     0      0   ...         0           0        0          0        0   \n",
      "\n",
      "   monstrous  searched  yell  abhor  jawbone  \n",
      "0          0         0     0      0        0  \n",
      "1          0         0     0      0        1  \n",
      "\n",
      "[2 rows x 4075 columns]\n"
     ]
    }
   ],
   "source": [
    "#Report feature vectors any two reviews\n",
    "print trainingVecs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.e Postprocessing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### L1 or lasso was chosen as the normalization method because it best suits sparse data.\n",
    "def normalize(df):\n",
    "    normalizedDf = preprocessing.normalize(df, norm='l1')\n",
    "    return pd.DataFrame(normalizedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalizedTraining = normalize(trainingVecs)\n",
    "normalizedTest = normalize(testVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.f Training set clustering (K-means implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFirstMeans(trainData,k):\n",
    "    firstMeans = []\n",
    "    idxs = [x for x in range(len(trainData))]\n",
    "    meanIdxs = random.sample(idxs,k)\n",
    "    for idx in meanIdxs:\n",
    "        firstMeans.append(trainData.iloc[idx].values)\n",
    "    return firstMeans\n",
    "\n",
    "def clustering(trainData,clusterMeans,k):\n",
    "    rowIdxs = [i for i in range(len(trainData))]\n",
    "    clusters = []\n",
    "    for num in range(k):\n",
    "        clusters.append([])\n",
    "    \n",
    "    for row in rowIdxs :\n",
    "        distances = []\n",
    "        for mean in clusterMeans:\n",
    "            dist = np.linalg.norm(trainData.iloc[row] - mean)\n",
    "            distances.append(dist)\n",
    "        clusterIdx = np.argmin(distances)\n",
    "        \n",
    "        clusters[clusterIdx].append(row)\n",
    "    return clusters\n",
    "    \n",
    "def newMeans(trainData,clusters):\n",
    "    means = []\n",
    "    for cluster in clusters:\n",
    "        clusterMean = np.mean(trainData.iloc[cluster])\n",
    "        means.append(clusterMean)\n",
    "    return means\n",
    "\n",
    "def KMeans(trainData,k):\n",
    "    iters = 0\n",
    "    initMeans = getFirstMeans(trainData,k)\n",
    "    clusters1 = clustering(trainData,initMeans,k)\n",
    "    means1 =  newMeans(trainData,clusters1)\n",
    "    clusters2 = clustering(trainData,means1,k)\n",
    "    means2 = newMeans(trainData,clusters2)\n",
    "    while clusters1 != clusters2:\n",
    "        print \"iteration: \" + str(iters+1)\n",
    "        clusters1 = clustering(trainData,means2,k)\n",
    "        means1 = newMeans(trainData,clusters1)\n",
    "        clusters2 = clustering(trainData,means1,k)\n",
    "        means2 = newMeans(trainData,clusters2)\n",
    "        iters += 1\n",
    "    \n",
    "    return clusters2,means2\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "kMeansCluster,kMeansClusterMean= KMeans(normalizedTraining,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      0\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "4045    0\n",
      "4046    0\n",
      "4047    0\n",
      "4048    0\n",
      "4049    0\n",
      "4050    0\n",
      "4051    0\n",
      "4052    0\n",
      "4053    0\n",
      "4054    0\n",
      "4055    0\n",
      "4056    0\n",
      "4057    0\n",
      "4058    0\n",
      "4059    0\n",
      "4060    0\n",
      "4061    0\n",
      "4062    0\n",
      "4063    0\n",
      "4064    0\n",
      "4065    0\n",
      "4066    0\n",
      "4067    0\n",
      "4068    0\n",
      "4069    0\n",
      "4070    0\n",
      "4071    0\n",
      "4072    0\n",
      "4073    0\n",
      "4074    0\n",
      "dtype: float64, 0       0.000178\n",
      "1       0.000042\n",
      "2       0.000052\n",
      "3       0.000060\n",
      "4       0.000077\n",
      "5       0.000030\n",
      "6       0.000070\n",
      "7       0.000073\n",
      "8       0.000133\n",
      "9       0.000084\n",
      "10      0.000719\n",
      "11      0.000581\n",
      "12      0.000419\n",
      "13      0.000060\n",
      "14      0.000038\n",
      "15      0.000030\n",
      "16      0.000120\n",
      "17      0.000115\n",
      "18      0.000047\n",
      "19      0.000022\n",
      "20      0.000038\n",
      "21      0.000510\n",
      "22      0.000014\n",
      "23      0.000223\n",
      "24      0.002515\n",
      "25      0.000047\n",
      "26      0.000042\n",
      "27      0.000070\n",
      "28      0.000042\n",
      "29      0.000358\n",
      "          ...   \n",
      "4045    0.000194\n",
      "4046    0.000038\n",
      "4047    0.000276\n",
      "4048    0.000018\n",
      "4049    0.000914\n",
      "4050    0.000035\n",
      "4051    0.001893\n",
      "4052    0.000047\n",
      "4053    0.000105\n",
      "4054    0.000082\n",
      "4055    0.000679\n",
      "4056    0.000060\n",
      "4057    0.000068\n",
      "4058    0.000342\n",
      "4059    0.000052\n",
      "4060    0.000035\n",
      "4061    0.000010\n",
      "4062    0.000052\n",
      "4063    0.000297\n",
      "4064    0.000030\n",
      "4065    0.000038\n",
      "4066    0.000114\n",
      "4067    0.000687\n",
      "4068    0.000105\n",
      "4069    0.000030\n",
      "4070    0.000017\n",
      "4071    0.000070\n",
      "4072    0.000070\n",
      "4073    0.000084\n",
      "4074    0.000433\n",
      "dtype: float64]\n"
     ]
    }
   ],
   "source": [
    "print len(kMeansCluster)\n",
    "print kMeansClusterMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def displayResults(dfTrain,clusters):\n",
    "    dfTrain[1] = dfTrain[1].map(int)\n",
    "    for i,cluster in enumerate(clusters):\n",
    "        clusterData = dfTrain.iloc[cluster]\n",
    "        clusterData[1] = clusterData[1].map(int)\n",
    "        print\n",
    "        print \"Number of points in cluster \" + str(i) + \" :\"  + str(len(cluster))\n",
    "        print \"Number of 1s in cluster \" + str(i) + \" :\" + str(clusterData[clusterData[1] == 1].shape[0])\n",
    "        print \"percent of 1s in  cluster \" + str(i) + \" :\" + str((clusterData[clusterData[1] == 1].shape[0])/float(len(cluster))) \n",
    "        print \"Number of 0s in cluster \" + str(i) + \" :\" + str(clusterData[clusterData[1] == 0].shape[0])\n",
    "        print \"percent of 0s in cluster \" +  str(i) + \" :\" + str((clusterData[clusterData[1] == 0].shape[0])/float(len(cluster)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of points in cluster 0 :11\n",
      "Number of 1s in cluster 0 :3\n",
      "percent of 1s in  cluster 0 :0.272727272727\n",
      "Number of 0s in cluster 0 :8\n",
      "percent of 0s in cluster 0 :0.727272727273\n",
      "\n",
      "Number of points in cluster 1 :2389\n",
      "Number of 1s in cluster 1 :1197\n",
      "percent of 1s in  cluster 1 :0.501046462955\n",
      "Number of 0s in cluster 1 :1192\n",
      "percent of 0s in cluster 1 :0.498953537045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescoperera/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "displayResults(trainDF,kMeansCluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.g Sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logisticRegression(trainVecs,trainLabels,testVecs,testLabels):\n",
    "    model = linear_model.LogisticRegression()\n",
    "    modelFit = model.fit(trainVecs, trainLabels)\n",
    "    score = modelFit.score(testVecs, testLabels)\n",
    "    pred = modelFit.predict(testVecs)\n",
    "    confusionMatrix = confusion_matrix(pred, testLabels)\n",
    "    W = model.coef_\n",
    "    return score,confusionMatrix,W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score,confusionMatrix,weightVec = logisticRegression(normalizedTraining,trainDF[1],normalizedTest,testDF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843333333333\n",
      "[[270  64]\n",
      " [ 30 236]]\n"
     ]
    }
   ],
   "source": [
    "print score\n",
    "print confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getInfluentialWords(W,vecList):\n",
    "    wordVector = vecList.loc[0].index\n",
    "    maxNegIdx = W.argmin()\n",
    "    maxPosIdx = W.argmax()\n",
    "    negWord = wordVector[maxNegIdx] \n",
    "    posWord = wordVector[maxPosIdx]\n",
    "    print \"The word that most impacts negative reviews is: \" + str(negWord)\n",
    "    print \"The word that most impacts positive reviews is: \" + str(posWord)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word that most impacts negative reviews is: bad\n",
      "The word that most impacts positive reviews is: great\n"
     ]
    }
   ],
   "source": [
    "getInfluentialWords(weightVec,trainingVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.h N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateBiGramDict(reviews):\n",
    "    \"\"\"\n",
    "    words is a list of words. The function creates a dictionary of unique words\n",
    "    \"\"\"\n",
    "    biGramDict ={}\n",
    "    for review in reviews:\n",
    "        for i in range(len(review)-1):\n",
    "            biGram= review[i] + \" \" + review[i+1]\n",
    "            #biGram = str(review)\n",
    "            if biGram not in  biGramDict.keys():\n",
    "                biGramDict[biGram] = 0\n",
    "    return biGramDict\n",
    "\n",
    "def indexBiGram(d):\n",
    "    \"\"\"\n",
    "    bag is the input. The function essentially assigns an index for each word in the bag.The index will be needed\n",
    "    for counting the frequency of its word and create the correct feature vector.\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    for biGram in d.keys():\n",
    "        d[biGram] = idx\n",
    "        idx+=1\n",
    "    return d\n",
    "\n",
    "def biGramFrequency(reviews,d):\n",
    "    \"\"\"\n",
    "    iterates through a review and using the bag, it returns the feature vector of the review.\n",
    "    \"\"\"\n",
    "    featureVectorList = []\n",
    "    for review in reviews:\n",
    "        featureVec = [0.0 for x in range(0,len(d.keys()))]\n",
    "        for i in range(len(review)-1):\n",
    "            biGram= review[i] + \" \" + review[i+1]\n",
    "            if biGram in d.keys():\n",
    "                #update the value in featureVec whose index is found in bag[word]\n",
    "                featureVec[d[biGram]]+=1.0\n",
    "            else:\n",
    "                print \"Error: \" + biGram + \" not in dictionary.\"\n",
    "        featureVectorList.append(featureVec)\n",
    "    return pd.DataFrame(featureVectorList,columns = d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate bag only from Training Data\n",
    "biGramDict = generateBiGramDict(trainDF[0].tolist())\n",
    "indexedBiGram = indexBiGram(biGramDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate all the feature vectors for Training and Test Data\n",
    "biGramTrainingVecs = biGramFrequency(trainDF[0].tolist(),indexedBiGram)\n",
    "biGramTestVecs = biGramFrequency(testDF[0].tolist(),indexedBiGram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tension medical  give chill  clock docking  scratched protective  \\\n",
      "0                0           0              0                     0   \n",
      "1                0           0              0                     0   \n",
      "\n",
      "   problem others  expression feeling  except cole  played period  \\\n",
      "0               0                   0            0              0   \n",
      "1               0                   0            0              0   \n",
      "\n",
      "   friend enjoy  eaten multiple     ...      lucy bell  phone mp3  slow take  \\\n",
      "0             0               0     ...              0          0          0   \n",
      "1             0               0     ...              0          0          0   \n",
      "\n",
      "   day week  must shakespear  touching character  barely lukewarm  headset pc  \\\n",
      "0         0                0                   0                0           0   \n",
      "1         0                0                   0                0           0   \n",
      "\n",
      "   later returned  prime time  \n",
      "0               0           0  \n",
      "1               0           0  \n",
      "\n",
      "[2 rows x 11173 columns]\n"
     ]
    }
   ],
   "source": [
    "#Report feature vectors any two reviews\n",
    "print biGramTrainingVecs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Logistic Regression on the normalized train and test vectors obtained from the Bi Gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizedBiGramTrainingVecs = normalize(biGramTrainingVecs)\n",
    "normalizedBiGramTestVecs = normalize(biGramTestVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biGramScore,biGramConfusionMatrix,biGramWeightVec = logisticRegression(normalizedBiGramTrainingVecs,trainDF[1],\n",
    "                                                       normalizedBiGramTestVecs,testDF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976666666667\n",
      "[[300  14]\n",
      " [  0 286]]\n"
     ]
    }
   ],
   "source": [
    "print biGramScore\n",
    "print biGramConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word that most impacts negative reviews is: waste time\n",
      "The word that most impacts positive reviews is: work great\n"
     ]
    }
   ],
   "source": [
    "getInfluentialWords(biGramWeightVec,biGramTrainingVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.i PCA for bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PCA Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Center the data by subtracting the mean from it.\n",
    "2. Calculate the covariance matrix.\n",
    "3. Calculate the eigenvectors of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center(x,mean):\n",
    "    for row in x:\n",
    "        row -= mean\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingMean = normalizedTraining.mean()\n",
    "testMean = normalizedTest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centeredTraining = center(normalizedTraining,trainingMean)\n",
    "centeredTest = center(normalizedTest,testMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute SVD to get the eigenvectors and the eigenvalues\n",
    "UTrain,DTrain, VTrain = scipy.linalg.svd(centeredTraining, full_matrices=False)\n",
    "UTest, DTest, VTest = scipy.linalg.svd(centeredTest, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 2400) (2400,) (2400, 4075)\n",
      "(600, 600) (600,) (600, 4075)\n"
     ]
    }
   ],
   "source": [
    "print UTrain.shape,DTrain.shape, VTrain.shape\n",
    "print UTest.shape, DTest.shape, VTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PCA(U,D,V,x):\n",
    "    \"\"\"\n",
    "    takes the SVD decomposition matrices and returns a lower dimensional representation of the data\n",
    "    \"\"\"\n",
    "    size = D.shape[0]\n",
    "    newD = np.zeros((size,size))\n",
    "    newDiagonal = np.concatenate((D[:x],[0] * (len(D) - x)))\n",
    "    i  = 0\n",
    "    j = 0\n",
    "    while i < size and j < size:\n",
    "        newD[i][j] = newDiagonal[i]\n",
    "        i+=1\n",
    "        j+=1\n",
    "    return pd.DataFrame(np.dot(U, np.dot(newD,V)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train10 = PCA(UTrain,DTrain,VTrain,10)\n",
    "train50 = PCA(UTrain,DTrain,VTrain,50)\n",
    "train100 = PCA(UTrain,DTrain,VTrain,100)\n",
    "\n",
    "test10 = PCA(UTest,DTest,VTest,10)\n",
    "test50 = PCA(UTest,DTest,VTest,50)\n",
    "test100 = PCA(UTest,DTest,VTest,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizedTrain10 = normalize(train10)\n",
    "normalizedTest10 = normalize(test10)\n",
    "\n",
    "normalizedTrain50 = normalize(train50)\n",
    "normalizedTest50 = normalize(test50)\n",
    "\n",
    "normalizedTrain100 = normalize(train100)\n",
    "normalizedTest100 = normalize(test100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "iteration: 1\n",
      "2 2\n",
      "iteration: 2\n",
      "2 2\n",
      "iteration: 3\n",
      "2 2\n",
      "iteration: 4\n",
      "2 2\n",
      "2 2\n",
      "iteration: 1\n",
      "2 2\n",
      "iteration: 2\n",
      "2 2\n",
      "2 2\n",
      "iteration: 1\n",
      "2 2\n",
      "iteration: 2\n",
      "2 2\n",
      "iteration: 3\n",
      "2 2\n",
      "iteration: 4\n",
      "2 2\n",
      "iteration: 5\n",
      "2 2\n",
      "iteration: 6\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "clusterNum = 2\n",
    "kMeansCluster10,kMeansClusterMean10= KMeans(normalizedTrain10,clusterNum)\n",
    "kMeansCluster50,kMeansClusterMean50= KMeans(normalizedTrain50,clusterNum)\n",
    "kMeansCluster100,kMeansClusterMean100= KMeans(normalizedTrain100,clusterNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of points in cluster 0 :168\n",
      "Number of 1s in cluster 0 :149\n",
      "percent of 1s in  cluster 0 :0.886904761905\n",
      "Number of 0s in cluster 0 :19\n",
      "percent of 0s in cluster 0 :0.113095238095\n",
      "\n",
      "Number of points in cluster 1 :2232\n",
      "Number of 1s in cluster 1 :1051\n",
      "percent of 1s in  cluster 1 :0.470878136201\n",
      "Number of 0s in cluster 1 :1181\n",
      "percent of 0s in cluster 1 :0.529121863799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescoperera/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "displayResults(trainDF,kMeansCluster10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of points in cluster 0 :104\n",
      "Number of 1s in cluster 0 :62\n",
      "percent of 1s in  cluster 0 :0.596153846154\n",
      "Number of 0s in cluster 0 :42\n",
      "percent of 0s in cluster 0 :0.403846153846\n",
      "\n",
      "Number of points in cluster 1 :2296\n",
      "Number of 1s in cluster 1 :1138\n",
      "percent of 1s in  cluster 1 :0.495644599303\n",
      "Number of 0s in cluster 1 :1158\n",
      "percent of 0s in cluster 1 :0.504355400697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescoperera/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "displayResults(trainDF,kMeansCluster50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of points in cluster 0 :85\n",
      "Number of 1s in cluster 0 :45\n",
      "percent of 1s in  cluster 0 :0.529411764706\n",
      "Number of 0s in cluster 0 :40\n",
      "percent of 0s in cluster 0 :0.470588235294\n",
      "\n",
      "Number of points in cluster 1 :2315\n",
      "Number of 1s in cluster 1 :1155\n",
      "percent of 1s in  cluster 1 :0.498920086393\n",
      "Number of 0s in cluster 1 :1160\n",
      "percent of 0s in cluster 1 :0.501079913607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescoperera/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "displayResults(trainDF,kMeansCluster100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score10,confusionMatrix10,weightVec10 = logisticRegression(normalizedTrain10,trainDF[1],normalizedTest10,testDF[1])\n",
    "score50,confusionMatrix50,weightVec50 = logisticRegression(normalizedTrain50,trainDF[1],normalizedTest50,testDF[1])\n",
    "score100,confusionMatrix100,weightVec100 = logisticRegression(normalizedTrain100,trainDF[1],normalizedTest100,testDF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.608333333333 0.7 0.596666666667\n"
     ]
    }
   ],
   "source": [
    "print score10,score50,score100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[209 144]\n",
      " [ 91 156]]\n",
      "[[242 122]\n",
      " [ 58 178]]\n",
      "[[185 127]\n",
      " [115 173]]\n"
     ]
    }
   ],
   "source": [
    "print confusionMatrix10\n",
    "print confusionMatrix50\n",
    "print confusionMatrix100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
